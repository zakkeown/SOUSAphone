# Model Comparison Sweep - STARTED

**Date:** 2026-02-02 07:57:55
**Status:** ðŸƒ RUNNING
**Log:** `/tmp/sweep_small_20epoch.log`
**PID:** 60434

## Configuration

- **Dataset:** small
- **Epochs:** 20 per model
- **Models:** AST, EfficientAT, HTS-AT, BEATs (4 total)
- **Strategy:** LoRA only
- **Augmentation:** Both with and without (2 settings)
- **Total experiments:** 8 (4 models Ã— 1 strategy Ã— 2 aug settings)

## Experiments

1. âœ… **AST + LoRA + aug** (Running...)
2. â³ **AST + LoRA + noaug**
3. â³ **EfficientAT + LoRA + aug**
4. â³ **EfficientAT + LoRA + noaug**
5. â³ **HTS-AT + LoRA + aug**
6. â³ **HTS-AT + LoRA + noaug**
7. â³ **BEATs + LoRA + aug**
8. â³ **BEATs + LoRA + noaug**

## Estimated Time

**Per experiment:** ~3-4 hours (20 epochs on small dataset)
**Total:** ~24-32 hours for all 8 experiments (running sequentially)

## Monitor Progress

```bash
# Watch live progress
tail -f /tmp/sweep_small_20epoch.log

# Check status
ps aux | grep "run_sweep.py"

# View results as they complete
ls -lht ~/Code/SOUSAphone/results/sweep/*.json | head -10

# Analyze partial results (works even while sweep is running)
cd ~/Code/SOUSAphone
/Users/zakkeown/.pyenv/versions/3.11.14/bin/python3 scripts/analyze_sweep.py
```

## Results Location

- **JSON results:** `~/Code/SOUSAphone/results/sweep/`
- **Error logs:** `~/Code/SOUSAphone/results/sweep/errors/`
- **Analysis:** Generated by `analyze_sweep.py` after completion

## What Was Fixed

All models now have correct PEFT target modules:

### AST (Audio Spectrogram Transformer)
```yaml
peft_target_modules:
  - query
  - key
  - value
  - output.dense
```
- Standard transformer architecture
- 98.45% parameter reduction
- Validated: âœ… PASS

### EfficientAT (MobileNetV3-based CNN)
```yaml
peft_target_modules:
  - attention_pool.0
  - attention_pool.2
  - classifier.1
```
- CNN architecture (LoRA only on Linear layers, not Conv2d)
- ~90% parameter reduction
- Validated: âœ… PASS

### HTS-AT (Hierarchical Swin Transformer)
```yaml
peft_target_modules:
  - attention.self.query
  - attention.self.key
  - attention.self.value
  - attention.output.dense
  - intermediate.dense
  - output.dense
```
- Uses separate Q/K/V (not fused qkv)
- 97.69% parameter reduction
- Validated: âœ… PASS

### BEATs (Self-supervised audio transformer)
```yaml
peft_target_modules:
  - attention.q_proj
  - attention.k_proj
  - attention.v_proj
  - attention.out_proj
  - feed_forward.intermediate_dense
  - feed_forward.output_dense
```
- Custom naming with _proj suffix
- 99.38% parameter reduction
- Validated: âœ… PASS

## Expected Results

Based on tiny dataset tests and literature:

### Best Overall Accuracy (predicted)
1. **BEATs** - 99.38% param reduction, self-supervised pretraining
2. **AST** - 98.45% param reduction, proven on AudioSet
3. **HTS-AT** - 97.69% param reduction, hierarchical attention
4. **EfficientAT** - ~90% param reduction, lightweight design

### Fastest Training (predicted)
1. **EfficientAT** - Small model (2.5M params), CNN-based
2. **HTS-AT** - Medium size (27.6M params)
3. **AST** - Large (86.2M params)
4. **BEATs** - Large (94.4M params)

### Most Parameter Efficient (predicted)
1. **BEATs** - 99.38% reduction (only 0.62% trainable)
2. **AST** - 98.45% reduction (only 1.55% trainable)
3. **HTS-AT** - 97.69% reduction (only 2.31% trainable)
4. **EfficientAT** - ~90% reduction (only ~10% trainable)

## Success Criteria

âœ… **All experiments complete without errors**
âœ… **Training loss decreases consistently**
âœ… **Validation accuracy > 40% (baseline: 2.5% for 40 classes)**
âœ… **Metrics properly logged and saved**

## If Sweep Fails

1. **Check logs:** `cat /tmp/sweep_small_20epoch.log | grep -i error`
2. **Check error directory:** `ls results/sweep/errors/`
3. **Resume sweep:** `/Users/zakkeown/.pyenv/versions/3.11.14/bin/python3 scripts/run_sweep.py --data small --max-epochs 20 --models ast efficientat htsat beats --strategies lora --resume`

## After Completion

### 1. Analyze Results
```bash
cd ~/Code/SOUSAphone
/Users/zakkeown/.pyenv/versions/3.11.14/bin/python3 scripts/analyze_sweep.py
```

This generates:
- `results/sweep/comparison_table.csv` - All metrics
- `results/sweep/plots/accuracy_comparison.png` - Bar chart
- `results/sweep/plots/efficiency_scatter.png` - Params vs accuracy
- `results/sweep/plots/training_time.png` - Time comparison
- `results/sweep/summary_report.md` - Human-readable analysis

### 2. Review Top Performer

Identify the best model from the summary, then:

```bash
# Train best model for production (50 epochs, full dataset)
cd ~/Code/SOUSAphone
/Users/zakkeown/.pyenv/versions/3.11.14/bin/python3 train.py \
  model=<best_model> \
  strategy=lora \
  data=small \
  training.max_epochs=50 \
  wandb.mode=online
```

### 3. Compare with Previous Failed Sweep

Previous sweep: 28 completed (with empty metrics), 26 failed
This sweep: Should be 8 completed (with full metrics), 0 failed

## Files Modified

- `configs/model/ast.yaml` - Added peft_target_modules
- `configs/model/efficientat.yaml` - Added peft_target_modules
- `configs/model/htsat.yaml` - Added peft_target_modules
- `configs/model/beats.yaml` - Added peft_target_modules

## Timeline

- **07:57:55** - Sweep started (Experiment 1/8: AST + LoRA + aug)
- **~11:57** - Experiment 1 complete (estimated)
- **~15:57** - Experiment 2 complete (estimated)
- **~19:57** - Experiment 3 complete (estimated)
- **~23:57** - Experiment 4 complete (estimated)
- **~03:57 (next day)** - Experiment 5 complete (estimated)
- **~07:57 (next day)** - Experiment 6 complete (estimated)
- **~11:57 (next day)** - Experiment 7 complete (estimated)
- **~15:57 (next day)** - All complete! (estimated)

**Total:** ~32 hours from start to finish

## Notes

- Sweep runs experiments sequentially (one at a time)
- Each experiment has 2-hour timeout (7200 seconds)
- Results saved incrementally (can analyze partial results anytime)
- W&B logging in offline mode (sync later with `wandb sync`)
- Git commit tracked in each result file for reproducibility

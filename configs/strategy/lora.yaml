type: lora
load_pretrained: true

# LoRA hyperparameters
rank: 8
alpha: 16
dropout: 0.1
target_modules:
  - "attention.query"
  - "attention.key"
  - "attention.value"

# LoRA needs higher LR than full fine-tuning (B matrix inits to zero)
# Published AST+LoRA research (Cappellazzo et al.) uses 5e-3
learning_rate: 1e-3

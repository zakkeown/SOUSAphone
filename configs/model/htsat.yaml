# configs/model/htsat_fixed.yaml
name: htsat
class_path: sousa.models.htsat.HTSATModel
num_classes: 40
pretrained: true

# Input requirements
# Pretrained laion/clap-htsat-unfused uses num_mel_bins=64
input_type: spectrogram
sample_rate: 16000
n_mels: 64
n_fft: 1024  # Use 1024 to avoid zero mel filterbanks
hop_length: 160
max_length: 256  # Matches CLAP spec_size

# Spectrogram normalization - USE SOUSA-SPECIFIC STATS
normalize_spec: true
norm_mean: 1.9421
norm_std: 4.3660

# PEFT target modules for HTS-AT (Swin Transformer variant)
# Despite being called "Swin", this implementation uses separate Q/K/V layers
# like standard transformers (not fused qkv like original Swin)
peft_target_modules:
  - attention.self.query
  - attention.self.key
  - attention.self.value
  - attention.output.dense
  - intermediate.dense
  - output.dense

# Classifier head must stay fully trainable (not frozen by PEFT)
peft_modules_to_save:
  - classifier

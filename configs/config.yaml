defaults:
  - model: ast
  - strategy: lora
  - data: tiny
  - _self_

# Training hyperparameters
training:
  max_epochs: 50
  learning_rate: 5e-5
  batch_size: 4  # Reduced to prevent memory overflow on 36GB machine
  gradient_accumulation_steps: 8  # Increased to maintain effective batch size of 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  label_smoothing: 0.1
  early_stopping_patience: 10

# Augmentation
augmentation:
  specaugment: true
  specaugment_freq_mask: 30
  specaugment_time_mask: 40
  specaugment_n_freq_masks: 2
  specaugment_n_time_masks: 2

  mixup: true
  mixup_alpha: 0.2

  time_stretch: false
  time_stretch_min: 0.8
  time_stretch_max: 1.25

# System
seed: 42
precision: "16-mixed"
accelerator: "auto"
num_workers: 0  # Use main process only to minimize memory (workers duplicate memory)

# Experiment tracking
wandb:
  project: "sousa-rudiment-classification"
  entity: null  # Set to your W&B username
  mode: "online"  # or "offline" for local dev
  tags: ["${model.name}", "${strategy.type}"]

# Paths
dataset_path: "~/Code/SOUSA/output/dataset"

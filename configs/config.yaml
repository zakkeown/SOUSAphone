defaults:
  - model: ast
  - strategy: lora
  - data: tiny
  - _self_

# Training hyperparameters
training:
  max_epochs: 50
  learning_rate: 5e-5
  batch_size: 8  # Reduced from 16 to prevent memory overflow
  gradient_accumulation_steps: 4  # Increased to maintain effective batch size of 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  label_smoothing: 0.1
  early_stopping_patience: 10

# Augmentation
augmentation:
  specaugment: true
  specaugment_freq_mask: 30
  specaugment_time_mask: 40
  specaugment_n_freq_masks: 2
  specaugment_n_time_masks: 2

  mixup: true
  mixup_alpha: 0.2

# System
seed: 42
precision: "16-mixed"
accelerator: "auto"
num_workers: 2  # Reduced from 4 to prevent memory overflow from prefetching

# Experiment tracking
wandb:
  project: "sousa-rudiment-classification"
  entity: null  # Set to your W&B username
  mode: "online"  # or "offline" for local dev
  tags: ["${model.name}", "${strategy.type}"]

# Paths
dataset_path: "~/Code/SOUSA/output/dataset"
